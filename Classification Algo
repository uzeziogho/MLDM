import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from pylab import *

import seaborn as sns

from collections import Counter

class1= pd.read_csv('/content/Maternal Health Risk Data Set.csv')
class1.head()

kf=class1.copy()
class1.describe()

class1.hist(layout=(4,4), figsize=(20,10))

sns.countplot(class1['RiskLevel'])

class1['RiskLevel'].unique

class1.corr(method='pearson')

X = class1[['Age','SystolicBP','DiastolicBP','BodyTemp','BS','HeartRate']]
y_Risk = class1[['RiskLevel']]

# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
y = labelencoder.fit_transform(y_Risk.values.ravel())
sns.countplot(y)

print(y_train.shape)
print(X_train.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# 1 Algorithm: Logistic Regression
lr = LogisticRegression(solver='newton-cg', class_weight={0:1.229,1:0.845,2:1.04})
lr.fit(X_train, y_train)
pred_lr = lr.predict(X_test)
print(y_test)
print(pred_lr)
print(metrics.accuracy_score(y_test, pred_lr)*100)
print(confusion_matrix(y_test, pred_lr))
print(classification_report(y_test, pred_lr))

#2 Algorithm: KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
pred_knn = knn.predict(X_test)
print(y_test)
print(pred_knn)
cm=metrics.confusion_matrix(y_test,pred_knn)
#print('accuracy:%.2f\n\n'%(acc))
print(metrics.accuracy_score(y_test, pred_knn)*100)
print(confusion_matrix(y_test, pred_knn))
print(classification_report(y_test, pred_knn))

ax=sns.heatmap(cm,cmap='flare',annot=True,fmt='d')

plt.xlabel("Predicted Class",fontsize=12)
plt.ylabel("True class,fontsize=12")
plt.title("Confusion Matrix",fontsize=12)
plt.show()

#3 Algorithm: Decision Tree
from sklearn.tree import DecisionTreeClassifier
classifier= DecisionTreeClassifier(criterion = 'entropy',random_state=42)
classifier.fit(X_train,y_train)

#predicting the test set results
y_pred=classifier.predict(X_test)
print(y_pred)
print(y_test)

cm=metrics.confusion_matrix(y_test,y_pred)
#print('accuracy:%.2f\n\n'%(acc))
print(metrics.accuracy_score(y_test, y_pred)*100)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn import metrics
acc=metrics.accuracy_score(y_test,y_pred)
print('accuracy:%.2f\n\n'%(acc))
cm=metrics.confusion_matrix(y_test,y_pred)
print('Confusion Matrix:')
print(cm,'\n\n')
print('----------------------------------------------------------')
result=metrics.classification_report(y_test,y_pred)
print(result)

ax=sns.heatmap(cm,cmap='flare',annot=True,fmt='d')

plt.xlabel("Predicted Class",fontsize=12)
plt.ylabel("True class,fontsize=12")
plt.title("Confusion Matrix",fontsize=12)
plt.show()

import tensorflow as tf

#Dependencies
import keras
from keras.models import Sequential
from keras.layers import Dense

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=99)
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)

# Neural network
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(6, ), activation='relu'))
#model.add(tf.keras.layers.Dense(6, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

0:1.229,1:0.845,2:1.04

class_weights={0:1.229,1:0.845,2:1.04}
history=model.fit(X_train,y_train,batch_size=8,epochs=50,class_weight=class_weights,verbose=2,validation_split=0.2)

accuracy=history.history['accuracy']
validation_accuracy=history.history['val_accuracy']

plt.plot(accuracy,label='Training Set Accuracy')
plt.plot(validation_accuracy,label='Validation Set Accuracy')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),0.6])
plt.title('Training and validation Accuracy Across Epochs')
plt.legend()

loss=history.history['loss']
validation_loss=history.history['val_loss']

plt.plot(loss,label='Training Set Loss')
plt.plot(validation_loss,label='Validation Set Loss')
plt.ylabel('Loss')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy Across Epochs')
plt.legend()

y_pred=model.predict(X_test)
y_pred=y_pred.argmax(axis=1)

cm=metrics.confusion_matrix(y_test,y_pred)

ax=sns.heatmap(cm,cmap='flare',annot=True,fmt='d')

plt.xlabel("Predicted Class",fontsize=12)
plt.ylabel("True class,fontsize=12")
plt.title("Confusion Matrix",fontsize=12)
plt.show()

print(classification_report(y_test,y_pred))
